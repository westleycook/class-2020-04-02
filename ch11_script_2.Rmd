---
title: 'Chapter 11: Univariate Regression'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(infer)
library(broom)
library(skimr)
library(gganimate)
library(tidyverse)

# Thanks to amazing CA Rucha Joshi for preparing this county dataset and for
# writing a draft of this script. All the good questions are due to her hard
# work! If she visits your group, give her a round of applause.

county <- read_rds("county.rds")
```

# Class Two

# Scene 8

**Prompt** Using the `lm()` function, fit a model with this data in which `poverty` is the dependent variable and `less_than_hs` is the independent variable. Save the resulting object as `poverty_model`. Then, use the tidy() function found in section 11.1.2 to obtain the regression parameters. You should have a 2x7 regression table in which the `term` variable has two values: "(Intercept)" and "less_than_hs". There are five other variables. Write one sentence explaining what the intercept means and one sentence about what the slope of the regression means. Chapter 11 of the *Primer* has lots of useful guidance.

```{r scene-8}

poverty_model <- county %>% 
  lm(poverty ~ less_than_hs, data = .)

poverty_model %>% 
  tidy(conf.int = TRUE)

# intercept: our estimate of the poverty rate if everyone has a hs diploma
# slope: for every 1% increase in less_than_hs, there's a .64% increase in poverty rate

```


# Scene 9

**Prompt** Use nest() to create a 1,000 bootstrap samples of the the data, just as we did when estimating confidence intervals. In each row of this tibble, we'll have a resampled collection of counties in which weâ€™ll sometimes have multiple counties represented and sometimes there will be counties that don't even appear. Use `cache=TRUE` in your R code chunk options since this takes time and you don't want to recalculate it each time. Save the resulting object as `county_bootstrap`.

```{r scene-9, cache=TRUE}

nreps = 100

county_bootstrap <- county %>% 
  rep_sample_n(size = nrow(county),
               reps = nreps,
               replace = TRUE) %>% 
  group_by(replicate) %>% 
  nest()

county_bootstrap %>% 
  mutate(distinct_rows = map_dbl(data, ~ n_distinct(.)))

```


When first creating this, or any other objecting with bootstrap resamples, it is smart to get everything working with three replicates before moving to n = 1000.

At this stage, `county_bootstrap` has two columns: `replicate` (an integer) and `data` (a list). Explore this object by going to the Environment pane and clicking on `county_bootstrap`. Normally, we don't explore objects starting from the Environment pane but list columns are confusing and this is an easy way to examine them. 

How can we check to make sure that the rows in `data` are different, as they should be if the bootstrap samples really are different? Add a third column, called `distinct_rows`, which is the the number of distinct rows  in `data` for each replicate. Hint: `n_distinct`. Recall that, when we work with list columns, like `data`, we use different map functions --- `map`, `map_dbl`, et cetera --- depending what our function is returning. And don't forget the tilde. Have you read [these](https://davidkane9.github.io/PPBDS/6-functions.html#using-map_-functions-to-create-list-columns) [parts](https://davidkane9.github.io/PPBDS/11-regression.html#uncertainty-in-simple-linear-regressions) of the *Primer* recently?

And, yes, it is somewhat awkward that `nest()` produces a column called `data` and that "data" is such a common term used in many places in R. We just need to keep track of things, even when they have the same names. Life is hard!



# Scene 10 

**Prompt**  Now, using the starter code above, go ahead and add more columns. Make one called `mod` which will contains the model objects created by `lm()`. Then, add one called `reg_results` which will tidy the objects created by `lm()`, and then one called `disp_coef` which will display the regression coefficient for each bootstrap sample. Is all this a mystery? Check out chapter 11 in the *Primer*. 

```{r scene-10}

reg_bootstrap <- county_bootstrap %>% 
  mutate(mod = map(data, ~ lm(poverty ~ less_than_hs, .)),
         reg_results = map(mod, ~ tidy(., conf.int = TRUE)),
         disp_coef = map_dbl(reg_results, 
                             ~ filter(., term == "less_than_hs") %>% 
                                  pull(estimate)))

```


# Scene 11 

**Prompt** Create a confidence interval for the slope of our linear regression. What is the value at the 50th percentile? Is that expected? What is the 95% confidence interval? Provide a Bayesian and Frequentist interpretation of this interval.

```{r scene-11}

reg_bootstrap %>% 
  pull(disp_coef) %>% 
  quantile(c(.5))

# .639

reg_bootstrap %>% 
  pull(disp_coef) %>% 
  quantile(c(.025, .975))

# (.591, .688)

# Frequentist: 95% of the time we run this process, the interval will yield the
# true slope of the regression line

# Bayesian: we are 95% confident that the true slope of the regression line lies
# within this interval

```


# Scene 12 

**Prompt** Now, let's use a shortcut. Use the confidence intervals reported by `lm()` and `tidy()`. How do these results compare with those from the previous scene? 

```{r scene-12}

county %>% 
  lm(poverty ~ less_than_hs, data = .) %>% 
  tidy(conf.int = TRUE)

```


# Scene 13

**Prompt** Alas, our data is missing Travis County in Texas. Suppose Travis County has 10.9% of adults with less than a high school degree. What do you think its poverty rate would be? Why? 

```{r scene-13}

yhat = estimate + slope * less_than_hs

6.635 + .637 * 10.9

# 13.578

```


# Scene 14

**Prompt** Suppose I tell you now that Travis County has a 12% poverty rate. By how much was your estimate off? Why?

```{r scene-14}

# Off by about 1.6% b/c there's variation in the data and our estimate might've
# been a little off (for both slope and intercept)

```

# Scene 15

**Prompt** Now, compute the fitted and residual values for each county. Explain what the following columns mean in one sentence each: poverty, pct_less_hs, .fitted, .resid. What does it mean to have a positive residual?
 
```{r scene-15}

county %>% 
  lm(poverty ~ less_than_hs, .) %>% 
  augment()

# poverty: poverty rate in original dataset
# less_than_hs: pct less than hs from original dataset
# .fitted: the expected value based on regression model
# .residual: the difference between actual and expected

# a positive residual indicates that a value was higher than predicted

```
 

# Scene 16

**Prompt** Find the largest positive residual and largest negative residual. Why do you think there are such large discrepancies?


# Challenge Problems

# Scene 1

**Prompt** Find the standard error of the fitted values, and then construct a confidence interval. Remember, a 95% confidence interval can be found by adding/subtracting 1.96 * SE to the mean. Why is the uncertainty for particular predictions higher than the uncertainty for our estimate of the coefficient on less_than_hs?


# Scene 2

**Prompt** Take a look at the babynames library. Create this animation: https://rpubs.com/ruchajoshi/bennetts

