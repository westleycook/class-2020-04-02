---
title: 'Chapter 11: Univariate Regression'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(infer)
library(broom)
library(skimr)
library(gganimate)
library(tidyverse)

# Thanks to amazing CA Rucha Joshi for preparing this county dataset and for
# writing a draft of this script. All the good questions are due to her hard
# work! If she visits your group, give her a round of applause.

county <- read_rds("county.rds")
```

# Class Two

# Scene 8

**Prompt** Using the `lm()` function, fit a model with this data in which `poverty` is the dependent variable and `less_than_hs` is the independent variable. Save the resulting object as `poverty_model`. Then, use the tidy() function found in section 11.1.2 to obtain the regression parameters. You should have a 2x7 regression table with terms of (Intercept) and `less_than_hs`, as well as an estimate, std.error, statistic, p.value, conf.low, and conf.high. Write one sentence explaining what the intercept means and one sentence about what the slope means.


# Scene 9

**Prompt** Use nest() to create a 1,000 bootstrap samples of the the data, just as we did when estimating confidence intervals. In each row of this tibble, we'll have a resampled collection of counties in which weâ€™ll sometimes have multiple counties represented and sometimes there will be counties that don't even appear. Use `cache=TRUE` in your R code chunk options since this takes time and you don't want to recalculate it each time. Save the resulting object as `county_bs`. We aren't, yet, doing anything with this object.

# Scene 10 

**Prompt**  Now, using the starter code above, go ahead and add more columns. Make one called `mod` which will contains the model objects created by `lm()`. Then, add one called `reg_results` which will tidy the objects created by `lm()`, and then one called `disp_coef` which will display the regression coefficient for each bootstrap sample. Is all this a mystery? Check out chapter 11 in the *Primer*. Recall that we use different map functions --- `map`, `map_dbl`, et cetera --- depending what our function is returning. And don't forget the tilde.


# Scene 11 

**Prompt** Create a confidence interval for the slope of our linear regression. What is the value at the 50th percentile? Is that expected? What is the 95% confidence interval? Provide a Bayesian and Frequentist interpretation of this interval.


# Scene 12 

**Prompt** Now, let's use a shortcut. Use the confidence intervals reported by `lm()` and `tidy()`. How do these results compare with those from the previous scene? 


# Scene 13

**Prompt** Alas, our data is missing Travis County in Texas. Suppose Travis County has 10.9% of adults with less than a high school degree. What do you think its poverty rate would be? Why? 

# Scene 14

**Prompt** Suppose I tell you now that Travis County has a 12% poverty rate. By how much was your estimate off? Why?

# Scene 15

**Prompt** Now, compute the fitted and residual values for each county. Explain what the following columns mean in one sentence each: poverty, pct_less_hs, .fitted, .resid. What does it mean to have a positive residual?

# Scene 16

**Prompt** Find the largest positive residual and largest negative residual. Why do you think there are such large discrepancies?


# Challenge Problems

# Scene 1

**Prompt** Find the standard error of the fitted values, and then construct a confidence interval. Remember, a 95% confidence interval can be found by adding/subtracting 1.96 * SE to the mean. Why is the uncertainty for particular predictions higher than the uncertainty for our estimate of the coefficient on less_than_hs?


# Scene 2

**Prompt** Take a look at the babynames library. Create this animation: https://rpubs.com/ruchajoshi/bennetts

